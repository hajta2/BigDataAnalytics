{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "statesDf = spark.read.csv(\n",
    "    \"./data/statesPopulation.csv\", header=True, inferSchema=True, sep=\",\"\n",
    ")\n",
    "statesDf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "statesDf.createOrReplaceTempView(\"states\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+\n",
      "|     State|Year|Population|\n",
      "+----------+----+----------+\n",
      "|California|2016|  39250017|\n",
      "|California|2015|  38993940|\n",
      "|California|2014|  38680810|\n",
      "|California|2013|  38335203|\n",
      "|California|2012|  38011074|\n",
      "+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statesDf.sort(\"population\", ascending=False).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+\n",
      "|     State|Year|Population|\n",
      "+----------+----+----------+\n",
      "|California|2016|  39250017|\n",
      "|California|2015|  38993940|\n",
      "|California|2014|  38680810|\n",
      "|California|2013|  38335203|\n",
      "|California|2012|  38011074|\n",
      "+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from states order by population desc limit 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    DateType,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 19|\n",
      "|Smith| 29|\n",
      "| Adam| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(\n",
    "    [Row(name=\"John\", age=19), Row(name=\"Smith\", age=29), Row(name=\"Adam\", age=35)]\n",
    ")\n",
    "schema = StructType(\n",
    "    [StructField(\"name\", StringType(), False), StructField(\"age\", IntegerType(), False)]\n",
    ")\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|first_name|       stats|\n",
      "+----------+------------+\n",
      "|    reggie| {10, 20, 5}|\n",
      "|       joe|{20, 30, 10}|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(\n",
    "    [\n",
    "        Row(first_name=\"reggie\", stats=Row(runs=10, hits=20, errors=5)),\n",
    "        Row(first_name=\"joe\", stats=Row(runs=20, hits=30, errors=10)),\n",
    "    ]\n",
    ")\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"first_name\", StringType(), False),\n",
    "        StructField(\n",
    "            \"stats\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"runs\", IntegerType(), False),\n",
    "                    StructField(\"hits\", IntegerType(), False),\n",
    "                    StructField(\"errors\", IntegerType(), False),\n",
    "                ]\n",
    "            ),\n",
    "            False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read csv file: 272.6104259490967 ms\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "statesDf = spark.read.csv(\n",
    "    \"./data/statesPopulation.csv\", header=True, inferSchema=True, sep=\",\"\n",
    ")\n",
    "stop = time.time()\n",
    "print(f\"Time taken to read csv file: {(stop - start) * 1000} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read the csv file: 42.0374870300293 ms\n",
      "+--------------------+----+----------+\n",
      "|               state|year|population|\n",
      "+--------------------+----+----------+\n",
      "|             Alabama|2010|   4785492|\n",
      "|              Alaska|2010|    714031|\n",
      "|             Arizona|2010|   6408312|\n",
      "|            Arkansas|2010|   2921995|\n",
      "|          California|2010|  37332685|\n",
      "|            Colorado|2010|   5048644|\n",
      "|            Delaware|2010|    899816|\n",
      "|District of Columbia|2010|    605183|\n",
      "|             Florida|2010|  18849098|\n",
      "|             Georgia|2010|   9713521|\n",
      "+--------------------+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "state_schema = StructType(\n",
    "    [\n",
    "        StructField(\"state\", StringType(), False),\n",
    "        StructField(\"year\", IntegerType(), False),\n",
    "        StructField(\"population\", IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "df_states = spark.read.csv(\n",
    "    \"./data/statesPopulation.csv\", header=True, schema=state_schema\n",
    ")\n",
    "stop = time.time()\n",
    "# print the time in milliseconds\n",
    "print(f\"Time taken to read the csv file: {(stop-start) * 1000} ms\")\n",
    "df_states.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read csv file: 299.5157241821289 ms\n",
      "+---+--------------------+----+---+----+\n",
      "|_c0|                 _c1| _c2|_c3| _c4|\n",
      "+---+--------------------+----+---+----+\n",
      "|  1|The Nightmare Bef...|1993|3.9|4568|\n",
      "|  2|           The Mummy|1932|3.5|4388|\n",
      "|  3|Orphans of the Storm|1921|3.2|9062|\n",
      "|  4|The Object of Beauty|1991|2.8|6150|\n",
      "|  5|          Night Tide|1963|2.8|5126|\n",
      "|  6| One Magic Christmas|1985|3.8|5333|\n",
      "|  7|    Muriel's Wedding|1994|3.5|6323|\n",
      "|  8|       Mother's Boys|1994|3.4|5733|\n",
      "|  9|Nosferatu: Origin...|1929|3.5|5651|\n",
      "| 10|        Nick of Time|1995|3.4|5333|\n",
      "+---+--------------------+----+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "movie_df = spark.read.csv(\n",
    "    \"./data/moviedata.csv\", header=False, inferSchema=True, sep=\",\"\n",
    ")\n",
    "stop = time.time()\n",
    "print(f\"Time taken to read csv file: {(stop - start) * 1000} ms\")\n",
    "movie_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read csv file: 20.772695541381836 ms\n",
      "+-------+--------------------+----+------+----------+\n",
      "|movieId|               title|year|rating|popularity|\n",
      "+-------+--------------------+----+------+----------+\n",
      "|      2|           The Mummy|1932|   3.5|      4388|\n",
      "|      3|Orphans of the Storm|1921|   3.2|      9062|\n",
      "|      4|The Object of Beauty|1991|   2.8|      6150|\n",
      "|      5|          Night Tide|1963|   2.8|      5126|\n",
      "|      6| One Magic Christmas|1985|   3.8|      5333|\n",
      "|      7|    Muriel's Wedding|1994|   3.5|      6323|\n",
      "|      8|       Mother's Boys|1994|   3.4|      5733|\n",
      "|      9|Nosferatu: Origin...|1929|   3.5|      5651|\n",
      "|     10|        Nick of Time|1995|   3.4|      5333|\n",
      "|     11|     Broken Blossoms|1919|   3.3|      5367|\n",
      "+-------+--------------------+----+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"movieId\", IntegerType(), False),\n",
    "        StructField(\"title\", StringType(), False),\n",
    "        StructField(\"year\", IntegerType(), False),\n",
    "        StructField(\"rating\", DoubleType(), False),\n",
    "        StructField(\"popularity\", IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "movie_df = spark.read.csv(\"./data/moviedata.csv\", header=True, schema=schema)\n",
    "stop = time.time()\n",
    "print(f\"Time taken to read csv file: {(stop - start) * 1000} ms\")\n",
    "movie_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+\n",
      "|     state|year|population|\n",
      "+----------+----+----------+\n",
      "|California|2016|  39250017|\n",
      "|California|2015|  38993940|\n",
      "|California|2014|  38680810|\n",
      "|California|2013|  38335203|\n",
      "|California|2012|  38011074|\n",
      "+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+----+----------+\n",
      "|     state|year|population|\n",
      "+----------+----+----------+\n",
      "|California|2016|  39250017|\n",
      "|California|2015|  38993940|\n",
      "|California|2014|  38680810|\n",
      "|California|2013|  38335203|\n",
      "|California|2012|  38011074|\n",
      "+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_states.createOrReplaceTempView(\"states\")\n",
    "df_states.sort(\"population\", ascending=False).show(5)\n",
    "spark.sql(\"select * from states order by population desc limit 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|    state|sum(population)|\n",
      "+---------+---------------+\n",
      "|     Utah|       20333580|\n",
      "|   Hawaii|        9810173|\n",
      "|Minnesota|       37914011|\n",
      "|     Ohio|       81020539|\n",
      "| Arkansas|       20703849|\n",
      "+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------------+\n",
      "|    state|sum(population)|\n",
      "+---------+---------------+\n",
      "|     Utah|       20333580|\n",
      "|   Hawaii|        9810173|\n",
      "|Minnesota|       37914011|\n",
      "|     Ohio|       81020539|\n",
      "| Arkansas|       20703849|\n",
      "+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_states.groupBy(\"state\").sum(\"population\").show(5)\n",
    "spark.sql(\"select state, sum(population) from states group by state\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|    state|   Total|\n",
      "+---------+--------+\n",
      "|     Utah|20333580|\n",
      "|   Hawaii| 9810173|\n",
      "|Minnesota|37914011|\n",
      "|     Ohio|81020539|\n",
      "| Arkansas|20703849|\n",
      "+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------+\n",
      "|    state|   Total|\n",
      "+---------+--------+\n",
      "|     Utah|20333580|\n",
      "|   Hawaii| 9810173|\n",
      "|Minnesota|37914011|\n",
      "|     Ohio|81020539|\n",
      "| Arkansas|20703849|\n",
      "+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_states.groupBy(\"state\").agg({\"population\": \"sum\"}).withColumnRenamed(\n",
    "    \"sum(population)\", \"Total\"\n",
    ").show(5)\n",
    "spark.sql(\"select state, sum(population) as Total from states group by state\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|    state|                 moy|\n",
      "+---------+--------------------+\n",
      "|     Utah|  2904797.1428571427|\n",
      "|   Hawaii|  1401453.2857142857|\n",
      "|Minnesota|   5416287.285714285|\n",
      "|     Ohio|1.1574362714285715E7|\n",
      "| Arkansas|   2957692.714285714|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|    state|                 moy|\n",
      "+---------+--------------------+\n",
      "|     Utah|  2904797.1428571427|\n",
      "|   Hawaii|  1401453.2857142857|\n",
      "|Minnesota|   5416287.285714285|\n",
      "|     Ohio|1.1574362714285715E7|\n",
      "| Arkansas|   2957692.714285714|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_states.groupBy(\"state\").agg({\"population\": \"avg\"}).withColumnRenamed(\n",
    "    \"avg(population)\", \"moy\"\n",
    ").show(5)\n",
    "spark.sql(\"select state, avg(population) as moy from states group by state\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|    state|     max|\n",
      "+---------+--------+\n",
      "|     Utah| 3051217|\n",
      "|   Hawaii| 1428557|\n",
      "|Minnesota| 5519952|\n",
      "|     Ohio|11614373|\n",
      "| Arkansas| 2988248|\n",
      "+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------+--------------------+--------+--------+\n",
      "|    state|   Total|                 moy|     min|     max|\n",
      "+---------+--------+--------------------+--------+--------+\n",
      "|     Utah|20333580|  2904797.1428571427| 2775326| 3051217|\n",
      "|   Hawaii| 9810173|  1401453.2857142857| 1363945| 1428557|\n",
      "|Minnesota|37914011|   5416287.285714285| 5311147| 5519952|\n",
      "|     Ohio|81020539|1.1574362714285715E7|11540983|11614373|\n",
      "| Arkansas|20703849|   2957692.714285714| 2921995| 2988248|\n",
      "+---------+--------+--------------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_states.groupBy(\"state\").agg(\n",
    "    {\"population\": \"sum\", \"population\": \"avg\", \"population\": \"min\", \"population\": \"max\"}\n",
    ").withColumnRenamed(\"sum(population)\", \"Total\").withColumnRenamed(\n",
    "    \"avg(population)\", \"moy\"\n",
    ").withColumnRenamed(\n",
    "    \"min(population)\", \"min\"\n",
    ").withColumnRenamed(\n",
    "    \"max(population)\", \"max\"\n",
    ").show(\n",
    "    5\n",
    ")\n",
    "spark.sql(\n",
    "    \"select state, sum(population) as Total, avg(population) as moy, min(population) as min, max(population) as max from states group by state\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|     state|    Total|\n",
      "+----------+---------+\n",
      "|California|268280590|\n",
      "|     Texas|185672865|\n",
      "|   Florida|137618322|\n",
      "|  New York|137409471|\n",
      "|  Illinois| 89960023|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------+\n",
      "|     state|    Total|\n",
      "+----------+---------+\n",
      "|California|268280590|\n",
      "|     Texas|185672865|\n",
      "|   Florida|137618322|\n",
      "|  New York|137409471|\n",
      "|  Illinois| 89960023|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_states.groupBy(\"state\").sum(\"population\").withColumnRenamed(\n",
    "    \"sum(population)\", \"Total\"\n",
    ").sort(\"Total\", ascending=False).show(5)\n",
    "spark.sql(\n",
    "    \"select state, sum(population) as Total from states group by state order by Total desc\"\n",
    ").show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
