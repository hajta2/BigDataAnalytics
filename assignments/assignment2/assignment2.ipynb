{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asssignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = {\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/26 12:15:21 WARN Utils: Your hostname, HP-Elite-845 resolves to a loopback address: 127.0.1.1; using 192.168.1.17 instead (on interface wlan0)\n",
      "23/05/26 12:15:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hajta2/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hajta2/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hajta2/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a546229c-b069-4442-8f69-9397669a121f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 582ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a546229c-b069-4442-8f69-9397669a121f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/15ms)\n",
      "23/05/26 12:15:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "spark = SparkSession.builder.appName('assignment3').config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = \"user-library-read\"\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyOAuth(scope=scope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send('tracks_topic', b'')\n",
    "producer.send('audio_features_topic', b'')\n",
    "producer.send('trending_tracks_topic', b'')\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"tracks_topic, audio_features_topic, trending_tracks_topic\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_stream = df.selectExpr(\"CAST(value AS STRING)\").filter(\"topic = 'tracks_topic'\")\n",
    "features_stream = df.selectExpr(\"CAST(value AS STRING)\").filter(\"topic = 'audio_features_topic'\")\n",
    "trending_tracks_stream = df.selectExpr(\"CAST(value AS STRING)\").filter(\"topic = 'trending_tracks_topic'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_query = tracks_stream.writeStream.format(\"memory\").queryName(\"tracks\").start()\n",
    "features_query = features_stream.writeStream.format(\"memory\").queryName(\"features\").start()\n",
    "trending_tracks_query = trending_tracks_stream.writeStream.format(\"memory\").queryName(\"trending_tracks\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_tracks = sp.current_user_saved_tracks(limit=50)\n",
    "for item in saved_tracks['items']:\n",
    "    track = item['track']\n",
    "    producer.send('tracks_topic', json.dumps(track).encode('utf-8'))\n",
    "    audio_features = sp.audio_features(track['id'])\n",
    "    producer.send('audio_features_topic', json.dumps(audio_features[0]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_playlist_id = sp.featured_playlists(limit=1, country='HU', locale='hu_HU')['playlists']['items'][0]['id']\n",
    "trending_tracks = sp.playlist_tracks(trending_playlist_id, limit=50, offset=0, market='HU')['items']\n",
    "for item in trending_tracks:\n",
    "    track = item['track']\n",
    "    producer.send('trending_tracks_topic', json.dumps(track).encode('utf-8'))\n",
    "    audio_features = sp.audio_features(track['id'])\n",
    "    producer.send('audio_features_topic', json.dumps(audio_features[0]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = spark.sql(\"select * from tracks\")\n",
    "features = spark.sql(\"select * from features\")\n",
    "trending_tracks = spark.sql(\"select * from trending_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks: 50\n",
      "Features: 99\n",
      "Trending tracks: 50\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tracks: {tracks.count()}\")\n",
    "print(f\"Features: {features.count()}\")\n",
    "print(f\"Trending tracks: {trending_tracks.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/26 12:15:38 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6f08c9d0] is aborting.\n",
      "23/05/26 12:15:38 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6f08c9d0] aborted.\n",
      "23/05/26 12:15:39 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2bc92aaa] is aborting.\n",
      "23/05/26 12:15:39 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2bc92aaa] aborted.\n"
     ]
    }
   ],
   "source": [
    "tracks_query.stop()\n",
    "features_query.stop()\n",
    "trending_tracks_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/26 12:15:39 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:464)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/26 12:15:39 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 224, attempt 0, stage 108.0)\n",
      "23/05/26 12:15:39 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 224, attempt 0, stage 108.0)\n",
      "23/05/26 12:15:39 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:464)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/26 12:15:39 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 226, attempt 0, stage 110.0)\n",
      "23/05/26 12:15:39 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 226, attempt 0, stage 110.0)\n"
     ]
    }
   ],
   "source": [
    "tracks.printSchema()\n",
    "features.printSchema()\n",
    "trending_tracks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"artists\", StringType(), True),\n",
    "        StructField(\"duration_ms\", IntegerType(), True),\n",
    "        StructField(\"popularity\", FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tracks_parsed = tracks.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\"), tracks_schema)\n",
    ").select(\"parsed_value.*\")\n",
    "trending_tracks_parsed = trending_tracks.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\"), tracks_schema)\n",
    ").select(\"parsed_value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- duration_ms: integer (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- duration_ms: integer (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracks_parsed.printSchema()\n",
    "trending_tracks_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks: 50\n",
      "Trending tracks: 50\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tracks: {tracks_parsed.count()}\")\n",
    "print(f\"Trending tracks: {trending_tracks_parsed.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+-----------+----------+\n",
      "|                  id| name|             artists|duration_ms|popularity|\n",
      "+--------------------+-----+--------------------+-----------+----------+\n",
      "|0pAiyIHt9DyHOjWgF...|Steal|[{\"external_urls\"...|     219945|      68.0|\n",
      "+--------------------+-----+--------------------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Tracks: None\n",
      "+--------------------+--------+--------------------+-----------+----------+\n",
      "|                  id|    name|             artists|duration_ms|popularity|\n",
      "+--------------------+--------+--------------------+-----------+----------+\n",
      "|7n5JBAnjVBTFgTEsd...|Inkassz√≥|[{\"external_urls\"...|     149673|      38.0|\n",
      "+--------------------+--------+--------------------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Trending tracks: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tracks: {tracks_parsed.show(1)}\")\n",
    "print(f\"Trending tracks: {trending_tracks_parsed.show(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"danceability\", FloatType(), True),\n",
    "        StructField(\"energy\", FloatType(), True),\n",
    "        StructField(\"key\", IntegerType(), True),\n",
    "        StructField(\"loudness\", FloatType(), True),\n",
    "        StructField(\"mode\", IntegerType(), True),\n",
    "        StructField(\"speechiness\", FloatType(), True),\n",
    "        StructField(\"acousticness\", FloatType(), True),\n",
    "        StructField(\"instrumentalness\", FloatType(), True),\n",
    "        StructField(\"liveness\", FloatType(), True),\n",
    "        StructField(\"valence\", FloatType(), True),\n",
    "        StructField(\"tempo\", FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "features_parsed = features.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\"), features_schema)\n",
    ").select(\"parsed_value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- danceability: float (nullable = true)\n",
      " |-- energy: float (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: float (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: float (nullable = true)\n",
      " |-- acousticness: float (nullable = true)\n",
      " |-- instrumentalness: float (nullable = true)\n",
      " |-- liveness: float (nullable = true)\n",
      " |-- valence: float (nullable = true)\n",
      " |-- tempo: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 99\n"
     ]
    }
   ],
   "source": [
    "print(f\"Features: {features_parsed.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+\n",
      "|                  id|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|\n",
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+\n",
      "|0pAiyIHt9DyHOjWgF...|       0.729| 0.456|  6|  -6.795|   0|     0.0528|        0.62|         1.33E-4|  0.0919|  0.221|110.999|\n",
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "Features: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Features: {features_parsed.show(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tracks = tracks_parsed.join(features_parsed, tracks_parsed.id == features_parsed.id).drop(features_parsed.id)\n",
    "joined_trending_tracks = trending_tracks_parsed.join(features_parsed, trending_tracks_parsed.id == features_parsed.id).drop(features_parsed.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined tracks: 50\n",
      "Joined trending tracks: 49\n"
     ]
    }
   ],
   "source": [
    "print(f\"Joined tracks: {joined_tracks.count()}\")\n",
    "print(f\"Joined trending tracks: {joined_trending_tracks.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- duration_ms: integer (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      " |-- danceability: float (nullable = true)\n",
      " |-- energy: float (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: float (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: float (nullable = true)\n",
      " |-- acousticness: float (nullable = true)\n",
      " |-- instrumentalness: float (nullable = true)\n",
      " |-- liveness: float (nullable = true)\n",
      " |-- valence: float (nullable = true)\n",
      " |-- tempo: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- duration_ms: integer (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      " |-- danceability: float (nullable = true)\n",
      " |-- energy: float (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: float (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: float (nullable = true)\n",
      " |-- acousticness: float (nullable = true)\n",
      " |-- instrumentalness: float (nullable = true)\n",
      " |-- liveness: float (nullable = true)\n",
      " |-- valence: float (nullable = true)\n",
      " |-- tempo: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_tracks.printSchema()\n",
    "joined_trending_tracks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"key\",\n",
    "    \"loudness\",\n",
    "    \"mode\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\"\n",
    "], outputCol=\"features\")\n",
    "assembled_tracks = assembler.transform(joined_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(assembled_tracks)\n",
    "scaled_tracks = scalerModel.transform(assembled_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"scaledFeatures\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\", predictionCol=\"prediction\", k=3, seed=1)\n",
    "model = kmeans.fit(scaled_tracks)\n",
    "predictions = model.transform(scaled_tracks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
