{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asssignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, lit, rand, udf, to_json, struct\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = {\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,com.redislabs:spark-redis_2.12:3.1.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hajta2/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hajta2/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hajta2/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.redislabs#spark-redis_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f40498ff-086a-4965-aaad-afe798515b27;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound com.redislabs#spark-redis_2.12;3.1.0 in central\n",
      "\tfound redis.clients#jedis;3.9.0 in central\n",
      ":: resolution report :: resolve 685ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.redislabs#spark-redis_2.12;3.1.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\tredis.clients#jedis;3.9.0 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;2.0.6] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   1   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f40498ff-086a-4965-aaad-afe798515b27\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/16ms)\n",
      "23/06/02 08:39:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"assignment3\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "    .config(\"spark.redis.port\", \"6379\")\n",
    "    .config(\"spark.redis.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "producer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = \"user-top-read playlist-modify-public playlist-modify-private\"\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyOAuth(scope=scope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send(\"liked_tracks_topic\", b\"\")\n",
    "producer.send(\"not_liked_tracks_topic\", b\"\")\n",
    "producer.send(\"trending_tracks_topic\", b\"\")\n",
    "producer.send(\"audio_features_topic\", b\"\")\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"liked_tracks_topic, audio_features_topic, trending_tracks_topic, not_liked_tracks_topic\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked_tracks_stream = df.selectExpr(\"CAST(value AS STRING)\").filter(\"topic = 'liked_tracks_topic'\")\n",
    "not_liked_tracks_stream = df.selectExpr(\"CAST(value AS STRING)\").filter(\"topic = 'not_liked_tracks_topic'\")\n",
    "features_stream = df.selectExpr(\"CAST(value AS STRING)\").filter(\n",
    "    \"topic = 'audio_features_topic'\"\n",
    ")\n",
    "trending_tracks_stream = df.selectExpr(\"CAST(value AS STRING)\").filter(\n",
    "    \"topic = 'trending_tracks_topic'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked_tracks_query = liked_tracks_stream.writeStream.format(\"memory\").queryName(\"liked_tracks\").start()\n",
    "not_liked_tracks_query = not_liked_tracks_stream.writeStream.format(\"memory\").queryName(\"not_liked_tracks\").start()\n",
    "features_query = (\n",
    "    features_stream.writeStream.format(\"memory\").queryName(\"features\").start()\n",
    ")\n",
    "trending_tracks_query = (\n",
    "    trending_tracks_stream.writeStream.format(\"memory\")\n",
    "    .queryName(\"trending_tracks\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "saved_tracks = sp.current_user_top_tracks(limit=50, offset=0, time_range=\"short_term\")\n",
    "for track in saved_tracks[\"items\"]:\n",
    "    producer.send(\"liked_tracks_topic\", json.dumps(track).encode(\"utf-8\"))\n",
    "    audio_features = sp.audio_features(track[\"id\"])\n",
    "    producer.send(\"audio_features_topic\", json.dumps(audio_features[0]).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "playlist_id = \"37i9dQZF1DX8SfyqmSFDwe\"\n",
    "not_liked_tracks = sp.playlist_tracks(playlist_id, limit=100, offset=0)\n",
    "for item in not_liked_tracks[\"items\"]:\n",
    "    track = item[\"track\"]\n",
    "    producer.send(\"not_liked_tracks_topic\", json.dumps(track).encode(\"utf-8\"))\n",
    "    audio_features = sp.audio_features(track[\"id\"])\n",
    "    producer.send(\"audio_features_topic\", json.dumps(audio_features[0]).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_playlist_id = sp.featured_playlists(limit=1, country=\"HU\", locale=\"hu_HU\")[\n",
    "    \"playlists\"\n",
    "][\"items\"][0][\"id\"]\n",
    "trending_tracks = sp.playlist_tracks(\n",
    "    trending_playlist_id, limit=50, offset=0, market=\"HU\"\n",
    ")[\"items\"]\n",
    "for item in trending_tracks:\n",
    "    track = item[\"track\"]\n",
    "    producer.send(\"trending_tracks_topic\", json.dumps(track).encode(\"utf-8\"))\n",
    "    audio_features = sp.audio_features(track[\"id\"])\n",
    "    producer.send(\"audio_features_topic\", json.dumps(audio_features[0]).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked_tracks = spark.sql(\"select * from liked_tracks\")\n",
    "not_liked_tracks = spark.sql(\"select * from not_liked_tracks\")\n",
    "features = spark.sql(\"select * from features\")\n",
    "trending_tracks = spark.sql(\"select * from trending_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks: 50\n",
      "Not liked tracks: 100\n",
      "Trending tracks: 49\n",
      "Features: 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tracks: {liked_tracks.count()}\")\n",
    "print(f\"Not liked tracks: {not_liked_tracks.count()}\")\n",
    "print(f\"Trending tracks: {trending_tracks.count()}\")\n",
    "print(f\"Features: {features.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked_tracks_query.stop()\n",
    "not_liked_tracks_query.stop()\n",
    "features_query.stop()\n",
    "trending_tracks_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "liked_tracks.printSchema()\n",
    "not_liked_tracks_stream.printSchema()\n",
    "features.printSchema()\n",
    "trending_tracks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"artists\", StringType(), True),\n",
    "        StructField(\"duration_ms\", IntegerType(), True),\n",
    "        StructField(\"popularity\", FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "liked_tracks_parsed = liked_tracks.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\"), tracks_schema)\n",
    ").select(\"parsed_value.*\")\n",
    "not_liked_tracks_parsed = not_liked_tracks.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\"), tracks_schema)\n",
    ").select(\"parsed_value.*\")\n",
    "trending_tracks_parsed = trending_tracks.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\"), tracks_schema)\n",
    ").select(\"parsed_value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- duration_ms: integer (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+-----------+----------+\n",
      "|                  id|                name|             artists|duration_ms|popularity|\n",
      "+--------------------+--------------------+--------------------+-----------+----------+\n",
      "|4Z4i631BesV0P6LTv...|Talk to Me You'll...|[{\"external_urls\"...|     417099|      55.0|\n",
      "+--------------------+--------------------+--------------------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Tracks: None\n"
     ]
    }
   ],
   "source": [
    "liked_tracks_parsed.printSchema()\n",
    "print(f\"Tracks: {liked_tracks_parsed.show(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"danceability\", FloatType(), True),\n",
    "        StructField(\"energy\", FloatType(), True),\n",
    "        StructField(\"key\", IntegerType(), True),\n",
    "        StructField(\"loudness\", FloatType(), True),\n",
    "        StructField(\"mode\", IntegerType(), True),\n",
    "        StructField(\"speechiness\", FloatType(), True),\n",
    "        StructField(\"acousticness\", FloatType(), True),\n",
    "        StructField(\"instrumentalness\", FloatType(), True),\n",
    "        StructField(\"liveness\", FloatType(), True),\n",
    "        StructField(\"valence\", FloatType(), True),\n",
    "        StructField(\"tempo\", FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "features_parsed = features.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\"), features_schema)\n",
    ").select(\"parsed_value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- danceability: float (nullable = true)\n",
      " |-- energy: float (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: float (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: float (nullable = true)\n",
      " |-- acousticness: float (nullable = true)\n",
      " |-- instrumentalness: float (nullable = true)\n",
      " |-- liveness: float (nullable = true)\n",
      " |-- valence: float (nullable = true)\n",
      " |-- tempo: float (nullable = true)\n",
      "\n",
      "Features: 200\n",
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+\n",
      "|                  id|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|\n",
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+\n",
      "|4Z4i631BesV0P6LTv...|       0.741| 0.619|  1| -11.366|   0|     0.0514|       0.661|           0.674|   0.125|  0.146|126.008|\n",
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "Features: None\n"
     ]
    }
   ],
   "source": [
    "features_parsed.printSchema()\n",
    "print(f\"Features: {features_parsed.count()}\")\n",
    "print(f\"Features: {features_parsed.show(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_liked_tracks = liked_tracks_parsed.join(\n",
    "    features_parsed, liked_tracks_parsed.id == features_parsed.id\n",
    ").drop(features_parsed.id)\n",
    "joined_not_liked_tracks = not_liked_tracks_parsed.join(\n",
    "    features_parsed, not_liked_tracks_parsed.id == features_parsed.id\n",
    ").drop(features_parsed.id)\n",
    "joined_trending_tracks = trending_tracks_parsed.join(\n",
    "    features_parsed, trending_tracks_parsed.id == features_parsed.id\n",
    ").drop(features_parsed.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_liked_tracks = joined_liked_tracks.withColumn(\"favorite\", lit(1))\n",
    "joined_not_liked_tracks = joined_trending_tracks.withColumn(\"favorite\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tracks = joined_liked_tracks.union(joined_not_liked_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tracks = combined_tracks.orderBy(rand())\n",
    "X_train, X_test = combined_tracks.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"duration_ms\",\n",
    "        \"popularity\",\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"key\",\n",
    "        \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    ")\n",
    "assembled_train = assembler.transform(X_train)\n",
    "assembled_test = assembler.transform(X_test)\n",
    "assembled_trending = assembler.transform(joined_trending_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(assembled_train)\n",
    "scaled_train = scalerModel.transform(assembled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(\n",
    "    featuresCol=\"features\", labelCol=\"favorite\", numTrees=100, maxDepth=5, maxBins=32\n",
    ")\n",
    "model = random_forest.fit(scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(assembled_test)\n",
    "print(f\"Precision: {predictions.filter(predictions.favorite == predictions.prediction).count() / predictions.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "ith = udf(ith_, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = model.transform(assembled_trending).filter(ith(col(\"probability\"), lit(1)) > 0.3).orderBy(col(\"probability\").desc()).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 333:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+-----------+----------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------------+--------------------+--------------------+----------+\n",
      "|                  id|          name|             artists|duration_ms|popularity|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------+--------------------+-----------+----------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------------+--------------------+--------------------+----------+\n",
      "|4sa05g6gxeYWeQlym...|Van pénzem, de|[{\"external_urls\"...|     163278|      15.0|       0.933| 0.523|  1|  -7.382|   0|     0.0715|       0.127|         0.00128|   0.123|  0.646|122.013|[163278.0,15.0,0....|[69.3883149300483...|[0.69388314930048...|       0.0|\n",
      "|6N7NO9Z1Hi6la8dqG...|   Lost at Sea|[{\"external_urls\"...|     187906|       0.0|       0.142| 0.188|  3| -13.704|   0|     0.0344|       0.957|            0.67|   0.118|  0.114| 167.87|[187906.0,0.0,0.1...|[69.0769230769230...|[0.69076923076923...|       0.0|\n",
      "|1q1qHGBqlmueSgrct...|  FANTOMREZGÉS|[{\"external_urls\"...|     145946|      41.0|       0.592| 0.514|  1| -16.194|   1|     0.0539|      0.0162|             0.0|   0.434|  0.496|106.934|[145946.0,41.0,0....|[66.1866425971689...|[0.66186642597168...|       0.0|\n",
      "|2xluJG2nDkkLeeb4q...|      PRESSURE|[{\"external_urls\"...|     259687|      60.0|       0.508|  0.59|  1|  -6.999|   1|      0.253|      0.0689|         1.57E-4|   0.142|  0.249|160.069|[259687.0,60.0,0....|[64.3882418909592...|[0.64388241890959...|       0.0|\n",
      "|4IOCrPntsn66ZdAQv...|       Freedom|[{\"external_urls\"...|     325161|       0.0|       0.653|  0.61|  7| -12.412|   1|      0.043|        0.55|           0.869|  0.0862| 0.0384|124.002|[325161.0,0.0,0.6...|[59.0317460317460...|[0.59031746031746...|       0.0|\n",
      "+--------------------+--------------+--------------------+-----------+----------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommendations_json = recommendations.select('id', 'name', 'artists', 'probability').toJSON().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = {}\n",
    "for track in recommendations_json:\n",
    "    track = json.loads(track)\n",
    "    artists = [artist[\"name\"] for artist in json.loads(track[\"artists\"])]\n",
    "    output_json[track[\"id\"]] = {\n",
    "        \"name\": track[\"name\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = recommendations.select('id', 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 360:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                  id|          name|\n",
      "+--------------------+--------------+\n",
      "|4sa05g6gxeYWeQlym...|Van pénzem, de|\n",
      "|6N7NO9Z1Hi6la8dqG...|   Lost at Sea|\n",
      "|1q1qHGBqlmueSgrct...|  FANTOMREZGÉS|\n",
      "|2xluJG2nDkkLeeb4q...|      PRESSURE|\n",
      "|4IOCrPntsn66ZdAQv...|       Freedom|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format('org.apache.spark.sql.redis').option('table', 'recommendations').option('key.column', 'id').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "readDF = spark.read.format(\"org.apache.spark.sql.redis\").option(\"table\", \"recommendations\").option('key.column', 'id').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                  id|          name|\n",
      "+--------------------+--------------+\n",
      "|6N7NO9Z1Hi6la8dqG...|   Lost at Sea|\n",
      "|4IOCrPntsn66ZdAQv...|       Freedom|\n",
      "|2xluJG2nDkkLeeb4q...|      PRESSURE|\n",
      "|1q1qHGBqlmueSgrct...|  FANTOMREZGÉS|\n",
      "|4sa05g6gxeYWeQlym...|Van pénzem, de|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
